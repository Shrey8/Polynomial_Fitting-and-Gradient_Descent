{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: Train the 2nd order polynomial predictor using both gradient descent and stochastic gradient descent. Optimize the stepsizes and compare against scikit-learn implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Download data from https://drive.google.com/file/d/0Bz9_0VdXvv9bUUNlUTVrMF9VcVU/view?usp=sharing.\n",
    "2. Create a function psi(x), which transforms features AST (assists), REB (rebounds) and STL (steals) into 2nd order polynomial features (add each feature squared and each pair of features multiplied with every other)\n",
    "3. Create a transformed data matrix X, where each x is mapped to psi(x).\n",
    "4. Create a function p2(x,w), which outputs the value of the polynomial at x for given parameters w.\n",
    "5. Create a function Loss(X,y,w), which computes the squared loss of predicting y from X by p2(x,w) using parameters w. Take variable PTS as y. We will predict scored points based on assists, rebounds and steals.\n",
    "6. Code up the gradient descent. It should input a point w and a stepsize.\n",
    "7. Choose an arbitrary point and stepsize. Run gradient descent for 100 iterations and compute the Loss after each iteration. How does the loss behave? Does it converge to something?\n",
    "8. Can you find the stepsize, for which the loss is smallest after 100 iterations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import pandas as pd\n",
    "import copy\n",
    "from ipykernel import kernelapp as app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = pd.read_csv(r\"/Users/Shrey/Downloads/nba_games_2013_2015 (1).csv\", delimiter=';')\n",
    "x = nb[['AST','REB','STL']].values\n",
    "y = nb[['PTS']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi(q):\n",
    "    f = [q[0]**2 , q[1]**2 , q[2]**2 , q[0]*q[1] , q[0]*q[2] , q[1]*q[2] ]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1681, 1849, 196, 1763, 574, 602]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi(x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in x:\n",
    "     X.append(psi(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1681, 1849,  196, 1763,  574,  602],\n",
       "       [ 529, 1849,   64,  989,  184,  344],\n",
       "       [ 400, 1521,   49,  780,  140,  273],\n",
       "       ...,\n",
       "       [ 529, 2704,   64, 1196,  184,  416],\n",
       "       [ 529, 1681,  121,  943,  253,  451],\n",
       "       [ 289, 1936,   16,  748,   68,  176]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "X.shape\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = np.array([[1,1,1,1,1,1]]).transpose()\n",
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p2(x,w):\n",
    "    # Take dot product of x values\n",
    "    return np.matmul(x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X,y,w):\n",
    "    total_loss = []\n",
    "    for i in X:\n",
    "        i = np.array([i])\n",
    "        loss = (y - p2(i,w))**2\n",
    "        total_loss.append(loss)\n",
    "    return sum(total_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss Function = $\\displaystyle\\frac{1}{2m}||Ax-b||_{2}^2$ or simply $J(\\theta) = 1/2m \\sum_{i=1}^{m} (h(\\theta)^{(i)} - y^{(i)})^2$\n",
    "\n",
    "Gradient = $\\displaystyle\\frac{1}{m}A^T(Ax-b)$ or simply $\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = 1/m\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_j^{(i)}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our loss function not convex won't converge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(y,X,learning_rate,iterations,theta):\n",
    "    #theta is initial start\n",
    "    past_thetas = []\n",
    "    past_costs = []\n",
    "    grad = []\n",
    "    for i in range(0,iterations):\n",
    "        #compute initial prediction\n",
    "        prediction = X.dot(theta)\n",
    "        #compute error \n",
    "        error = np.sum((y - prediction)**2)/(2*len(y))\n",
    "        #add error\n",
    "        past_costs.append(error)\n",
    "        #update parameters using gradient descent -> for least squares formula is X^t(Xw-Y)\n",
    "        gradient = (X.transpose().dot((X.dot(theta) - y)))/len(y)\n",
    "        grad.append(gradient)\n",
    "        theta = theta - learning_rate*gradient\n",
    "        past_thetas.append(theta)\n",
    "        \n",
    "        \n",
    "    return past_thetas, past_costs , grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_thetas, past_costs , grad = gradient_descent(y,X,0.0001,10,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(past_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing Stochastic Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(y,X,learning_rate,iterations,theta):\n",
    "    past_thetas = []\n",
    "    past_costs = []\n",
    "    grad = []\n",
    "    m = len(y)\n",
    "    for j in range(0,iterations):\n",
    "        cost = 0\n",
    "        for i in range(0,m):\n",
    "            rand = np.random.randint(0,m)\n",
    "            X_i = X[rand,:].reshape(1,X.shape[1])\n",
    "            y_i = y[rand].reshape(1,1)\n",
    "            prediction = np.dot(X_i,theta)\n",
    "            gradient = (X_i.T.dot((prediction - y_i)))/m\n",
    "            grad.append(gradient)\n",
    "            theta = theta -learning_rate*gradient\n",
    "            past_thetas.append(theta)\n",
    "            error = np.sum((y_i - prediction)**2)/(2*m)\n",
    "            past_costs.append(error)\n",
    "            \n",
    "    return past_thetas , past_costs , grad   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:17: RuntimeWarning: overflow encountered in square\n",
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: RuntimeWarning: invalid value encountered in subtract\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "past_thetas , past_costs , grad = stochastic_gradient_descent(y,X,0.01,10,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[109]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_i = X[7,:].reshape(1,X.shape[1])\n",
    "y_i = y[7].reshape(1,1)\n",
    "y_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73800"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(past_costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7380"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our Loss Function is equal to $\\displaystyle\\frac{1}{2m}||Ax-b||_{2}^2$ we need to find a x that minimizes the error. If we're lucky then our unique x will be equal to $(X^TX)^{-1}X^Ty$ and this is only possible if  $(X^TX)$ is invertible. Gradient descent doesn't always mean we will reach our global optimum so if we our data isn't too large we can solve this directly \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.05828381],\n",
       "       [ 0.0230394 ],\n",
       "       [ 0.13298273],\n",
       "       [ 0.01246142],\n",
       "       [-0.03234214],\n",
       "       [ 0.02218657]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Unique solution\n",
    "unique = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose().dot(y))\n",
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
